# Advanced LLM Diagnosis System - Setup Guide

Jetson Orin Nano Superç”¨ã®é«˜åº¦ãªLLMè¨ºæ–­ã‚·ã‚¹ãƒ†ãƒ ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰

## æ¦‚è¦

ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã¯ã€ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹è¨ºæ–­ã®ä¿¡é ¼åº¦ãŒä½ã„å ´åˆï¼ˆâ‰¤60%ï¼‰ã«ã€LLMã‚’ä½¿ç”¨ã—ãŸé«˜åº¦ãªè¨ºæ–­ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

### ä¸»è¦æ©Ÿèƒ½

1. **ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹è¨ºæ–­** (< 1ms)
   - æ—¢å­˜ã®4ã¤ã®ãƒ«ãƒ¼ãƒ«ã«ã‚ˆã‚‹é«˜é€Ÿè¨ºæ–­
   - ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®è¨ˆç®—

2. **RAGã‚·ã‚¹ãƒ†ãƒ **
   - Spotãƒãƒ‹ãƒ¥ã‚¢ãƒ«PDFã‹ã‚‰é–¢é€£æƒ…å ±ã‚’æ¤œç´¢
   - åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã«ã‚ˆã‚‹æ„å‘³æ¤œç´¢
   - FAISS ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ã‚ˆã‚‹é«˜é€Ÿæ¤œç´¢

3. **LLMè¨ºæ–­** (~3sec)
   - llama.cpp ã«ã‚ˆã‚‹è»½é‡æ¨è«–
   - ãƒãƒ‹ãƒ¥ã‚¢ãƒ«æƒ…å ± + RoboPoseç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ
   - ä½ä¿¡é ¼åº¦ã‚±ãƒ¼ã‚¹ã®ã¿å®Ÿè¡Œï¼ˆé¸æŠçš„èµ·å‹•ï¼‰

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
å…¥åŠ›ï¼ˆè„šçŠ¶æ…‹ï¼‰
    â†“
ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹è¨ºæ–­ â†’ ä¿¡é ¼åº¦è¨ˆç®—
    â†“
ä¿¡é ¼åº¦ > 60%? â†’ YES â†’ çµæœã‚’è¿”ã™
    â†“ NO
RAGã§ãƒãƒ‹ãƒ¥ã‚¢ãƒ«æ¤œç´¢
    â†“
LLMãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
    â†“
LLMæ¨è«– (Llama 3.2 3B)
    â†“
æœ€çµ‚è¨ºæ–­çµæœ
```

## å®Ÿè£…æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«

### ã‚³ã‚¢ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

1. **`rag_manual.py`** (æ–°è¦ä½œæˆ)
   - `ManualRAG` ã‚¯ãƒ©ã‚¹: PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã€åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã€æ¤œç´¢
   - `get_manual_rag()`: ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¢ã‚¯ã‚»ã‚¹

2. **`llm_advanced.py`** (æ–°è¦ä½œæˆ)
   - `AdvancedLLMAnalyzer` ã‚¯ãƒ©ã‚¹: LLMè¨ºæ–­ã‚¨ãƒ³ã‚¸ãƒ³
   - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ã€LLMå‘¼ã³å‡ºã—ã€çµæœãƒ‘ãƒ¼ã‚¹
   - `get_llm_analyzer()`: ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¢ã‚¯ã‚»ã‚¹

3. **`llm_client.py`** (æ›´æ–°)
   - `infer_with_confidence()` ãƒ¡ã‚½ãƒƒãƒ‰è¿½åŠ 
   - ç¢ºç‡åˆ†å¸ƒã®æœ€å¤§å€¤ã‚’ä¿¡é ¼åº¦ã¨ã—ã¦è¿”ã™

4. **`pipeline.py`** (æ›´æ–°)
   - LLMè¨ºæ–­çµ±åˆ
   - ä¿¡é ¼åº¦ãƒã‚§ãƒƒã‚¯ â†’ ä½ä¿¡é ¼åº¦æ™‚ã«LLMèµ·å‹•

5. **`config.py`** (æ›´æ–°)
   ```python
   USE_LLM_ADVANCED = False  # LLMè¨ºæ–­ã®æœ‰åŠ¹åŒ–
   LLM_CONFIDENCE_THRESHOLD = 0.6  # ä¿¡é ¼åº¦é–¾å€¤
   LLM_MODEL_PATH = "models/llama-3.2-3b-instruct-q4_k_m.gguf"
   MANUAL_PDF_PATH = "/home/kk21053/sotuken/Spot_IFU-v2.1.2-ja.pdf"
   MANUAL_EMBEDDINGS_CACHE = "data/manual_embeddings"
   ```

### ãƒ†ã‚¹ãƒˆãƒ»ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- **`test_llm_advanced.py`** (æ–°è¦ä½œæˆ): çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- **`requirements_llm.txt`** (æ–°è¦ä½œæˆ): ä¾å­˜é–¢ä¿‚ãƒªã‚¹ãƒˆ
- **`LLM_ADVANCED_SETUP.md`** (ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«): ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰

## ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †

### å‰ææ¡ä»¶

- Jetson Orin Nano Super (8GB RAM)
- JetPack 5.x ã¾ãŸã¯ 6.x
- Python 3.8+
- CUDA 11.4+ (JetPackã«å«ã¾ã‚Œã‚‹)

### 1. ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
cd /home/kk21053/sotuken/webots

# åŸºæœ¬ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
pip install -r requirements_llm.txt

# llama-cpp-python (CUDAæœ‰åŠ¹åŒ–)
CMAKE_ARGS="-DGGML_CUDA=ON" pip install llama-cpp-python
```

**æ³¨æ„**: Jetsonä¸Šã§ãƒ“ãƒ«ãƒ‰ã«ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼ˆ20-30åˆ†ï¼‰ã€‚

### 2. ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

```bash
# ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
mkdir -p models
cd models

# Llama 3.2 3B Instruct (Q4_K_Mé‡å­åŒ–ç‰ˆ)
# ã‚ªãƒ—ã‚·ãƒ§ãƒ³1: Hugging Faceã‹ã‚‰ç›´æ¥
wget https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf \
  -O llama-3.2-3b-instruct-q4_k_m.gguf

# ã‚ªãƒ—ã‚·ãƒ§ãƒ³2: åˆ¥ãƒã‚·ãƒ³ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¾Œã€scpã§è»¢é€
# scp llama-3.2-3b-instruct-q4_k_m.gguf jetson@<IP>:/home/kk21053/sotuken/webots/models/
```

**ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º**: ç´„2.5GB

### 3. åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆåˆå›è‡ªå‹•ï¼‰

åˆå›å®Ÿè¡Œæ™‚ã€`sentence-transformers` ãŒè‡ªå‹•ã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™:
- ãƒ¢ãƒ‡ãƒ«: `paraphrase-multilingual-MiniLM-L12-v2`
- ã‚µã‚¤ã‚º: ç´„500MB
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥: `~/.cache/torch/sentence_transformers/`

### 4. è¨­å®šã®æœ‰åŠ¹åŒ–

`controllers/diagnostics_pipeline/config.py` ã‚’ç·¨é›†:

```python
# LLMè¨ºæ–­ã‚’æœ‰åŠ¹åŒ–
USE_LLM_ADVANCED = True

# ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’ç¢ºèªï¼ˆå¿…è¦ã«å¿œã˜ã¦å¤‰æ›´ï¼‰
LLM_MODEL_PATH = "models/llama-3.2-3b-instruct-q4_k_m.gguf"
MANUAL_PDF_PATH = "/home/kk21053/sotuken/Spot_IFU-v2.1.2-ja.pdf"
```

### 5. ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ

```bash
cd /home/kk21053/sotuken/webots

# çµ±åˆãƒ†ã‚¹ãƒˆ
python test_llm_advanced.py
```

**æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›**:

```
TEST 1: RAG System (PDF Manual Search)
âœ“ RAG initialized with XXX chunks
...
âœ… RAG test passed

TEST 2: Confidence Calculation
...
âœ… Confidence calculation test passed

TEST 3: LLM Diagnosis
âœ“ LLM analyzer initialized
...
âœ… LLM diagnosis test passed

TEST 4: Pipeline Integration
...
âœ… Integration test passed

ğŸ‰ All tests passed!
```

## ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬çš„ãªä½¿ã„æ–¹

è¨ºæ–­ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯è‡ªå‹•çš„ã«LLMã‚’ä½¿ç”¨ã—ã¾ã™ï¼ˆ`USE_LLM_ADVANCED=True`ã®å ´åˆï¼‰:

```python
from diagnostics_pipeline.pipeline import DiagnosticsPipeline

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–ï¼ˆLLMè‡ªå‹•ãƒ­ãƒ¼ãƒ‰ï¼‰
pipeline = DiagnosticsPipeline(session_id="test_001")

# è¨ºæ–­å®Ÿè¡Œï¼ˆä¿¡é ¼åº¦ãŒä½ã„å ´åˆã€è‡ªå‹•ã§LLMèµ·å‹•ï¼‰
# ... æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã¨åŒã˜
```

### ãƒ­ã‚°å‡ºåŠ›ä¾‹

```
[pipeline] Initializing advanced LLM analyzer...
[llm_advanced] Loading model: models/llama-3.2-3b-instruct-q4_k_m.gguf
[llm_advanced] Model loaded successfully
[rag] Loading cached embeddings
[rag] Loaded 1234 chunks from cache
[pipeline] Advanced LLM enabled (confidence threshold: 0.6)

...

[pipeline] FL: Low confidence (45.2%), invoking LLM...
[llm_advanced] Running LLM diagnosis for FL...
[llm_advanced] Parsed distribution: {...}
[pipeline] FL: LLM diagnosis complete (new confidence: 87.3%)
```

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

### Jetson Orin Nano Super å®Ÿæ¸¬å€¤ï¼ˆäºˆæ¸¬ï¼‰

| å‡¦ç† | æ™‚é–“ | ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ |
|------|------|--------------|
| ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹è¨ºæ–­ | < 1ms | æœ€å° |
| RAGæ¤œç´¢ | ~50ms | ~500MB (åˆå›ã®ã¿) |
| LLMæ¨è«– | ~3ç§’ | ~3GB |
| **åˆè¨ˆï¼ˆLLMä½¿ç”¨æ™‚ï¼‰** | **~3ç§’** | **~3.5GB** |

### ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–

- åŸ‹ã‚è¾¼ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥: åˆå›ç”Ÿæˆå¾Œã¯ãƒ‡ã‚£ã‚¹ã‚¯ã‹ã‚‰èª­ã¿è¾¼ã¿
- ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³: ãƒ¢ãƒ‡ãƒ«ã¯1ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ã¿
- Q4é‡å­åŒ–: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’1/4ã«å‰Šæ¸›

### ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ

- ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ã¿: **600+ legs/sec**
- LLMä½µç”¨ï¼ˆ10%ãŒLLMèµ·å‹•ï¼‰: **~3 legs/sec** (LLM bottleneck)

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### 1. llama-cpp-python ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¤±æ•—

**ç—‡çŠ¶**: CUDAé–¢é€£ã®ãƒ“ãƒ«ãƒ‰ã‚¨ãƒ©ãƒ¼

**è§£æ±ºç­–**:
```bash
# CUDAãƒ‘ã‚¹ã‚’æ˜ç¤º
export CUDA_HOME=/usr/local/cuda
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH

CMAKE_ARGS="-DGGML_CUDA=ON" pip install llama-cpp-python --no-cache-dir
```

### 2. ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼

**ç—‡çŠ¶**: `CUDA out of memory`

**è§£æ±ºç­–**:
- ã‚ˆã‚Šå°ã•ã„ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨: Llama 3.2 1B (Q4_K_M)
- GPUå±¤æ•°ã‚’å‰Šæ¸›: `config.py` ã§ `n_gpu_layers = 20`
- ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å‰Šæ¸›ï¼ˆå®Ÿè£…æ¸ˆã¿: batch_size=1ï¼‰

### 3. PDFã‹ã‚‰åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆã§ããªã„

**ç—‡çŠ¶**: `[rag] Warning: PDF not found`

**è§£æ±ºç­–**:
```bash
# PDFãƒ‘ã‚¹ã‚’ç¢ºèª
ls -lh /home/kk21053/sotuken/Spot_IFU-v2.1.2-ja.pdf

# config.pyã®ãƒ‘ã‚¹ã‚’æ›´æ–°
MANUAL_PDF_PATH = "/æ­£ã—ã„ãƒ‘ã‚¹/Spot_IFU-v2.1.2-ja.pdf"
```

### 4. æ¨è«–ãŒé…ã„

**ç—‡çŠ¶**: LLMè¨ºæ–­ã«5ç§’ä»¥ä¸Šã‹ã‹ã‚‹

**è§£æ±ºç­–**:
- GPUå±¤æ•°ã‚’å¢—ã‚„ã™: `n_gpu_layers = 33`ï¼ˆå…¨å±¤ï¼‰
- CUDAæœ‰åŠ¹åŒ–ã‚’ç¢ºèª: `llama-cpp-python` ã®ãƒ“ãƒ«ãƒ‰ãƒ­ã‚°
- ã‚ˆã‚Šå°ã•ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: `n_ctx = 1024`

### 5. ãƒ†ã‚¹ãƒˆã§ "LLM not available"

**ç—‡çŠ¶**: `âš ï¸ LLM not available`

**åŸå› **: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„

**è§£æ±ºç­–**:
```bash
# ãƒ¢ãƒ‡ãƒ«ã®å­˜åœ¨ç¢ºèª
ls -lh models/llama-3.2-3b-instruct-q4_k_m.gguf

# ãƒ¢ãƒ‡ãƒ«ã‚’å†ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †2å‚ç…§ï¼‰
```

## é«˜åº¦ãªè¨­å®š

### ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨

ä»–ã®GGUFãƒ¢ãƒ‡ãƒ«ã‚‚ä½¿ç”¨å¯èƒ½:

```python
# config.py
LLM_MODEL_PATH = "models/your-custom-model.gguf"

# pipelineåˆæœŸåŒ–æ™‚
from diagnostics_pipeline.llm_advanced import get_llm_analyzer
analyzer = get_llm_analyzer(model_path="models/your-model.gguf")
```

### ä¿¡é ¼åº¦é–¾å€¤ã®èª¿æ•´

```python
# config.py
LLM_CONFIDENCE_THRESHOLD = 0.5  # ã‚ˆã‚Šå¤šãã®ã‚±ãƒ¼ã‚¹ã§LLMèµ·å‹•
LLM_CONFIDENCE_THRESHOLD = 0.8  # LLMèµ·å‹•ã‚’æŠ‘åˆ¶ï¼ˆé«˜é€ŸåŒ–ï¼‰
```

### RAGãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

```python
# rag_manual.py ã® ManualRAG.__init__()
rag = ManualRAG(
    chunk_size=300,       # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºç¸®å°ï¼ˆç²¾åº¦å‘ä¸Šï¼‰
    chunk_overlap=50,     # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—å‰Šæ¸›ï¼ˆé€Ÿåº¦å‘ä¸Šï¼‰
    model_name="paraphrase-multilingual-mpnet-base-v2",  # ã‚ˆã‚Šé«˜ç²¾åº¦ãªãƒ¢ãƒ‡ãƒ«
)
```

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

### GPUæœ€é©åŒ–

```python
# llm_advanced.py
llm = Llama(
    model_path=model_path,
    n_gpu_layers=33,      # å…¨å±¤ã‚’GPUã«ï¼ˆJetson Orin Nano Superã®å ´åˆï¼‰
    n_ctx=2048,           # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚º
    n_batch=512,          # ãƒãƒƒãƒã‚µã‚¤ã‚º
    n_threads=4,          # CPUã‚¹ãƒ¬ãƒƒãƒ‰æ•°
)
```

### ãƒ¡ãƒ¢ãƒª vs é€Ÿåº¦ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

| è¨­å®š | ãƒ¡ãƒ¢ãƒª | é€Ÿåº¦ | ç²¾åº¦ |
|------|--------|------|------|
| Q4_K_M, n_ctx=2048, å…¨å±¤GPU | 3GB | é€Ÿã„ | é«˜ |
| Q4_K_M, n_ctx=1024, å…¨å±¤GPU | 2GB | æœ€é€Ÿ | ä¸­ |
| Q4_K_M, n_ctx=2048, 20å±¤GPU | 2GB | ä¸­ | é«˜ |
| Q2_K, n_ctx=1024, å…¨å±¤GPU | 1.5GB | é€Ÿã„ | ä½ |

## é–‹ç™ºè€…å‘ã‘æƒ…å ±

### ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ 

```
webots/
â”œâ”€â”€ controllers/
â”‚   â””â”€â”€ diagnostics_pipeline/
â”‚       â”œâ”€â”€ llm_advanced.py       # LLMè¨ºæ–­ã‚¨ãƒ³ã‚¸ãƒ³
â”‚       â”œâ”€â”€ rag_manual.py         # RAGã‚·ã‚¹ãƒ†ãƒ 
â”‚       â”œâ”€â”€ llm_client.py         # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ + ä¿¡é ¼åº¦
â”‚       â”œâ”€â”€ pipeline.py           # çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
â”‚       â””â”€â”€ config.py             # è¨­å®š
â”œâ”€â”€ models/                       # LLMãƒ¢ãƒ‡ãƒ«ï¼ˆ.ggufï¼‰
â”œâ”€â”€ data/
â”‚   â””â”€â”€ manual_embeddings/        # åŸ‹ã‚è¾¼ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥
â”‚       â”œâ”€â”€ manual_index.json     # ãƒãƒ£ãƒ³ã‚¯æƒ…å ±
â”‚       â””â”€â”€ manual_embeddings.npy # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«
â”œâ”€â”€ test_llm_advanced.py          # ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ requirements_llm.txt          # ä¾å­˜é–¢ä¿‚
â””â”€â”€ LLM_ADVANCED_SETUP.md         # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
```

### API
